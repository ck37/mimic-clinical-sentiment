{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence sentiment\n",
    "\n",
    "This is run after 02_notes-to-sentences.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1602424774424,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "rNqLIArs85P1",
    "outputId": "c8d395fb-a4fe-4818-8cc3-1aaafb1a9bdc"
   },
   "outputs": [],
   "source": [
    "import os, re, time, sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "# Ensure numpy is single-core, so that parallel processing does not conflict for cpu.\n",
    "# This is needed because otherwise stanza will use multiple cores.\n",
    "max_threads = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = max_threads \n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = max_threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = max_threads\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = max_threads \n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = max_threads\n",
    "#import mkl\n",
    "#mkl.set_num_threads(int(max_threads))\n",
    "\n",
    "import pyprojroot\n",
    "import pandas as pd, numpy as np\n",
    "#print(\"Numpy BLAS:\", np.__config__.show())\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import medspacy, spacy\n",
    "from medspacy.ner import TargetRule \n",
    "from medspacy.visualization import visualize_ent\n",
    "\n",
    "# Parallel processing.\n",
    "import psutil \n",
    "num_cores = psutil.cpu_count(logical = False)\n",
    "print(\"CPU cores found:\", num_cores)\n",
    "from p_tqdm import p_map\n",
    "\n",
    "# Project-specific modules.\n",
    "from keywords import find_keywords2\n",
    "#from clinical_sectionizer import TextSectionizer, Sectionizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created in 02_notes-to-sentences.ipynb\n",
    "sent_df = pd.read_feather(\"data/mimic-sentences-pysbd.feather\")\n",
    "\n",
    "# 29 MM sentences with note category exclusions and age >= 18.\n",
    "print(sent_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "399285 in sent_df['row_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move into a function.\n",
    "# Load keywords (phrases)\n",
    "all_keywords = []\n",
    "neg_keywords = pd.read_csv('data-raw/negative_keywords.csv').iloc[::, 0]\n",
    "pos_keywords = pd.read_csv('data-raw/positive_keywords.csv').iloc[::, 0]\n",
    "\n",
    "all_keywords.append(neg_keywords.apply(lambda x: x.strip()).tolist())\n",
    "all_keywords.append(pos_keywords.apply(lambda x: x.strip()).tolist())\n",
    "\n",
    "# Convert to a dictionary temporarily to deduplicate keyword list (\"concerning\" is duplicated)\n",
    "keywords = list(dict.fromkeys([i.lower() for i in all_keywords for i in i]))\n",
    "negative_keywords=[i.strip().lower() for i in neg_keywords]\n",
    "positive_keywords=[i.strip().lower() for i in pos_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keywords import find_keywords2\n",
    "\n",
    "def find_keywords_df(df, keywords):\n",
    "    results = []\n",
    "    for row in df.itertuples():\n",
    "        found_dict = find_keywords2(row.text, keywords = keywords)\n",
    "        \n",
    "        # Convert dictionary with locations to just a list of found keywords/phrases.\n",
    "        # We don't need the location right now.\n",
    "        result = list(itertools.chain.from_iterable([[key_i] * len(locations) for key_i, locations in found_dict.items()]))\n",
    "\n",
    "        # Join original df back onto result.\n",
    "        #result2 = pd.join(row, result)\n",
    "        results.append(result)\n",
    "    combined = pd.DataFrame(pd.Series(results))\n",
    "    combined.index = df.index\n",
    "    return combined\n",
    "\n",
    "find_keywords_df(sent_df[:30], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes 55 mins with 24 cores on Benten.\n",
    "# Takes 40 mins on ssbvape.\n",
    "\n",
    "num_partitions = 2000\n",
    "#num_partitions = 5000\n",
    "#num_partitions = 10000\n",
    "\n",
    "# This will be a list of dfs.\n",
    "df_split = np.array_split(sent_df, num_partitions)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Apply our function to each dataframe chunk in the list.\n",
    "# Use partial() to specify the keywords argument.\n",
    "result = p_map(partial(find_keywords_df, keywords = keywords), df_split, num_cpus = num_cores)\n",
    "combined_kw = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_kw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_kw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_kw.sample(20, random_state = 1))\n",
    "\n",
    "# Shoulud be the same size as sent_df (30.2 MM)\n",
    "print(\"Combined kw length:\", len(combined_kw))\n",
    "sent_df['keywords'] = combined_kw[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate this before saving the above feather file.\n",
    "sent_df['keyword_count'] = sent_df['keywords'].str.len()\n",
    "sent_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "399285 in sent_df['row_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sent_df.reset_index(drop = True).to_feather(\"data/mimic-sentences-pysbd-kw.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that it works (or load from scratch if skipping above cells.\n",
    "sent_df = pd.read_feather(\"data/mimic-sentences-pysbd-kw.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_df.info())\n",
    "sent_df.head()\n",
    "\n",
    "print(sent_df[130:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "399285 in sent_df['row_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Score: Stanza, Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function needs to be defined in a file to be correctly exported\n",
    "# to the p_map parallel works, otherwise it will give an error.\n",
    "import sentiment # This refers to our sentiment.py module (file), not a pip package.\n",
    "from pattern import en\n",
    "\n",
    "# Calculate sentiment scores for multiple rows in a dataframe and return\n",
    "# a dataframe of results with the same number of rows and index.\n",
    "def score_sentences_df(df):\n",
    "    results = []\n",
    "    for row in df.itertuples():\n",
    "        # \n",
    "        result = {\n",
    "            # Stanza seems much closer than pattern, presumably because\n",
    "            # it's a neural model (and we've disabled GPU currently).\n",
    "            'sent_stanza': sentiment.sentiment_stanza(row.text),\n",
    "            'sent_pattern': en.sentiment(row.text)[0]\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "    combined = pd.DataFrame(results)\n",
    "    combined.index = df.index\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentiment_stanza() test:\",\n",
    "      sentiment.sentiment_stanza(sent_df.sample(1, random_state = 2).text.values[0]))\n",
    "\n",
    "print(\"Pattern test:\",\n",
    "      en.sentiment(sent_df.sample(1, random_state = 2).text.values[0])[0])\n",
    "\n",
    "print(\"\\nScore_sentences_df() test:\\n\",\n",
    "    score_sentences_df(sent_df.sample(20, random_state = 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes 6 hours 17 mins with 24 cores - make sure to run single-threaded.\n",
    "\n",
    "num_partitions = 10000\n",
    "\n",
    "# This will be a list of dfs.\n",
    "df_split = np.array_split(sent_df, num_partitions)\n",
    "\n",
    "result = p_map(score_sentences_df, df_split, num_cpus = num_cores)\n",
    "\n",
    "combined_df = pd.concat(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.shape)\n",
    "print(combined_df.head(20))\n",
    "print(combined_df.corr())\n",
    "\n",
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = sent_df.join(combined_df)\n",
    "sent_df.reset_index(drop = True).to_feather(\"data/mimic-sentences-sentiment-prelim.feather\")\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "399285 in sent_df['row_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score: DeBERTA-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "#from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# At least 1 gpu is needed for this to run reasonably quickly.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"GPU device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model, tokenizer, pipe\n",
    "import gc\n",
    "#del raw_dataset_sample\n",
    "#del sent_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "import datasets\n",
    "print(\"Datasets version:\", datasets.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, pipeline, TextClassificationPipeline\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "# Created in deep-learning-model.ipynb\n",
    "model_path = str(pyprojroot.here() / \"models/deberta-v3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\");\n",
    "print(\"GPU or CPU device:\", device)\n",
    "#model.to(device);\n",
    "\n",
    "# num_labels appears to mean num_classes\n",
    "# Need to put model on GPU for inference.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                           problem_type = \"single_label_classification\",\n",
    "                                                           num_labels = 5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from deep-learning-model.ipynb\n",
    "class_names = ['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive']\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "\n",
    "# Text should be a single string, not a vector currently.\n",
    "def predict_sentiment(text, return_class = True, max_length = 512):\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    # get output probabilities by doing softmax\n",
    "    probs = outputs[0].softmax(1)\n",
    "    #print(probs)\n",
    "    # executing argmax function to get the candidate label\n",
    "    if return_class:\n",
    "        return class_names[probs.argmax()]\n",
    "    else:\n",
    "        return probs.argmax()\n",
    "\n",
    "# Expected results: Neutral, Negative, Negative, Positive\n",
    "print(predict_sentiment(\"This is a test sentence.\"))\n",
    "print(predict_sentiment(\"I'm worried that the patient is doing poorly.\"))\n",
    "print(predict_sentiment(\"I'm extremely worried that the patient is doing terribly and will certainly die soon.\"))\n",
    "print(predict_sentiment(\"Patient's bp is normalizing, and kidney function appears to be improving.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentences, convert to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Takes 18 seconds to load.\n",
    "# CK: this was the wrong feather file to load it seems.\n",
    "#sent_df = pd.read_feather(\"data/mimic-sentences-sentiment.feather\")\n",
    "sent_df = pd.read_feather(\"data/mimic-sentences-sentiment-prelim.feather\")\n",
    "\n",
    "# Exclude ['row_id', 'sent_num'] to avoid errors later on, which is unfortunate.\n",
    "raw_dataset = Dataset.from_pandas(sent_df[['text']])\n",
    "print(\"Raw dataset:\\n\", raw_dataset)\n",
    "\n",
    "raw_dataset_sample = raw_dataset.from_pandas(sent_df[['text']].sample(1024).reset_index(drop = True))\n",
    "print(\"Raw dataset sample:\\n\", raw_dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "399285 in sent_df['row_id'].values\n",
    "del sent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset tokenization (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes ~37 minutes with a single core or 8 minutes with 6 cores.\n",
    "\n",
    "# Copied from deep-learning-model.ipynb and modified.\n",
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch['text'], truncation = True, padding = True, max_length = 256)\n",
    "    #result = labels.str2int(batch['labels'])\n",
    "    #tokens['labels'] = result\n",
    "    return tokens\n",
    "\n",
    "# Don't use num_cores - each process takes up too much RAM apparently.\n",
    "# We lose the progress bar when parallelized though :/\n",
    "tokenized_datasets = raw_dataset.map(tokenize, batched = True, num_proc = 6,\n",
    "            # Remove any extra columns to avoid a warning when training, not essential though.\n",
    "                                 remove_columns = raw_dataset.column_names)\n",
    "\n",
    "tokenized_datasets.set_format('torch')\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets.features)\n",
    "\n",
    "# This will create a subfolder rather than a single file.\n",
    "tokenized_datasets.save_to_disk(\"data/mimic-sentences-tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load tokenized dataset, create dataloaders (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized dataset.\n",
    "tokenized_datasets = load_from_disk(\"data/mimic-sentences-tokenized\")\n",
    "\n",
    "tokenized_dataloader = torch.utils.data.DataLoader(tokenized_datasets, batch_size = 8)\n",
    "\n",
    "# Version of dataloader that does not include tokenization.\n",
    "# Is there any usage of this though? To be determined.\n",
    "raw_dataloader = torch.utils.data.DataLoader(raw_dataset, batch_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline + Keydataset v2 (works?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes approx 24 hours.\n",
    "\n",
    "# device = 0 puts the pipeline on GPU, otherwise it will only use CPU.\n",
    "pipe = pipeline(\"text-classification\", model = model, tokenizer = tokenizer, device = 0, max_length = 256,\n",
    "               truncation = True)\n",
    "\n",
    "# Hide the large number of deprecation warnings.\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "\n",
    "preds = []\n",
    "# Progress bar is inaccurate for some reason :(\n",
    "# Batch size of 512 is close to the max GPU RAM usage - led to an OOM error eventually.\n",
    "# GPU RAM usage continues to grow through inference :( Something is not being deleted correctly.\n",
    "#for i, outputs in enumerate(tqdm(pipe(KeyDataset(raw_dataset, \"text\"), batch_size = 32),\n",
    "#    total = len(raw_dataset))):\n",
    "for outputs in tqdm(pipe(KeyDataset(raw_dataset, \"text\"), batch_size = 128),\n",
    "    total = len(raw_dataset)):\n",
    "#for i, outputs in enumerate(tqdm(pipe(KeyDataset(raw_dataset_sample, \"text\"), batch_size = 128),\n",
    "#    total = len(raw_dataset_sample))):\n",
    "#for i, outputs in enumerate(pipe(KeyDataset(raw_dataset_sample, \"text\"), batch_size = 128)):\n",
    "    #print(i)\n",
    "#    preds.append(outputs['score'])\n",
    "    preds.append(outputs['label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds)\n",
    "preds_df.rename(columns = {0: 'label'}, inplace = True)\n",
    "preds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_df.sample(10))\n",
    "# 1 = Negative, 2 = Neutral, 3 = Positive\n",
    "print(Counter(preds_df['label']))\n",
    "\n",
    "preds_df['label'] = preds_df['label'].astype(\"category\")\n",
    "print(preds_df.label.cat.categories)\n",
    "preds_df['label'] = preds_df.label.cat.rename_categories(['negative', 'neutral', 'positive'])\n",
    "print(preds_df.label.cat.categories)\n",
    "\n",
    "print(Counter(preds_df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save preds, convert LABEL_2, LABEL_3, etc. to the numeric codes.\n",
    "# Save note-level sentence average.\n",
    "preds_df.to_feather('data/debertav3-sent.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df['sent_deberta'] = preds_df.label.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df[['sent_stanza', 'sent_pattern', 'sent_deberta']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.reset_index(drop = True).to_feather(\"data/mimic-sentences-sentiment.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.read_feather(\"data/mimic-sentences-sentiment.feather\")\n",
    "sent_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "399285 in sent_df['row_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate to note averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df2 = sent_df.groupby(\"row_id\").agg(\n",
    "    # Will need to add 1 to this column.\n",
    "    sentences = pd.NamedAgg(column = \"sent_num\", aggfunc = \"max\"),\n",
    "    chars = pd.NamedAgg(column = \"chars\", aggfunc = \"sum\"),\n",
    "    words = pd.NamedAgg(column = \"words\", aggfunc = \"sum\"),\n",
    "    sent_stanza = pd.NamedAgg(column = 'sent_stanza', aggfunc = 'mean'),\n",
    "    sent_pattern = pd.NamedAgg(column = 'sent_pattern', aggfunc = 'mean'),\n",
    "    sent_deberta = pd.NamedAgg(column = 'sent_deberta', aggfunc = 'mean'))\n",
    "\n",
    "sent_df2['sentences'] = sent_df2['sentences'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df2[['sent_stanza', 'sent_pattern', 'sent_deberta']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df2.reset_index().to_feather('data/mimic-notes-sentiment.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_df2 = pd.read_feather('data/mimic-notes-sentiment.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#399285 in sent_df2['row_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvLib6eAZojVgTuuMLAZ1a",
   "collapsed_sections": [],
   "name": "MIMIC Notes Demo-Chris.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
