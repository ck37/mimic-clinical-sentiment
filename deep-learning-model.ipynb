{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32dcbbda-93f8-4822-9301-97543175bd2a",
   "metadata": {},
   "source": [
    "# Train deep learning sentiment model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdd640-3222-4905-80b7-5abc7d50bf77",
   "metadata": {},
   "source": [
    "## Check setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ef2e0d-2fb6-48e6-a8b8-d4638e213adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.10.1\n",
      "GPU available: True\n",
      "GPU device count: 2\n",
      "CUDA version: 11.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# At least 1 gpu is needed for this to run reasonably quickly.\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"GPU device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e5040-ac1d-4b29-b8b7-ea703a9c0629",
   "metadata": {},
   "source": [
    "## Import labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1c92de-0bef-43b8-93c6-c8e55285a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found labels file: True\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1499 entries, 0 to 1498\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   text               1499 non-null   object\n",
      " 1   aspect             1387 non-null   object\n",
      " 2   overall-sentiment  1493 non-null   object\n",
      " 3   aspect1-sentiment  1386 non-null   object\n",
      " 4   aspect2-sentiment  556 non-null    object\n",
      " 5   aspect3-sentiment  207 non-null    object\n",
      " 6   aspect4-sentiment  96 non-null     object\n",
      " 7   aspect5-sentiment  45 non-null     object\n",
      "dtypes: object(8)\n",
      "memory usage: 93.8+ KB\n",
      "Counter({'Positive': 649, 'Negative': 540, 'Neutral': 232, 'Very Positive': 45, 'Very Negative': 27, nan: 6})\n",
      "Counter({'Positive': 649, 'Negative': 540, 'Neutral': 232, 'Very Positive': 45, 'Very Negative': 27})\n"
     ]
    }
   ],
   "source": [
    "import pyprojroot\n",
    "data_file = pyprojroot.here() / \"data-raw/sentence-labels.csv\"\n",
    "import os\n",
    "print(\"Found labels file:\", os.path.exists(data_file))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(data_file)\n",
    "df.info()\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(df['overall-sentiment']))\n",
    "\n",
    "# We have a few nan values here.\n",
    "df.dropna(subset = ['overall-sentiment'], inplace = True)\n",
    "\n",
    "print(Counter(df['overall-sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e35a3-4c59-4377-aa74-68f2072ece91",
   "metadata": {},
   "source": [
    "## Create training/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28c4cfa-3113-4b3b-b121-0acac30acb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'train': 1194, 'test': 299})\n"
     ]
    }
   ],
   "source": [
    "# Create training and test splits.\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "shuffle = np.random.permutation(df.index)\n",
    "n_train = int(len(df) * 0.8)\n",
    "df['split'] = \"\"\n",
    "# use 'n_train' samples for training and the rest for testing\n",
    "train_ids = shuffle[:n_train]\n",
    "test_ids = shuffle[n_train:]\n",
    "\n",
    "df.loc[train_ids, \"split\"] = \"train\"\n",
    "df.loc[test_ids, \"split\"] = \"test\"\n",
    "\n",
    "df['split'] = df['split'].astype(\"category\")\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(df['split']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d8b0d-1a96-4d96-b365-8584ae97ee20",
   "metadata": {},
   "source": [
    "## Export training/test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3353cb1d-f84e-49bf-b9e4-a46e2e5a5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export into files.\n",
    "data_dir = pyprojroot.here() / \"data\"\n",
    "train_file = data_dir / \"labels-train.csv\"\n",
    "test_file = data_dir / \"labels-test.csv\"\n",
    "\n",
    "df2 = df.rename(columns = {'overall-sentiment': 'labels'})\n",
    "# We only need to export these two columns.\n",
    "columns = ['text', 'labels']\n",
    "\n",
    "df2.loc[df2['split'] == 'train', columns].to_csv(train_file, index = False)\n",
    "df2.loc[df2['split'] == 'test', columns].to_csv(test_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f7805-c0f8-477a-9d9f-edac75f94de0",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15209e3e-d9ac-44a8-afd7-f8dd00787742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0ff93346614de202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/ck37/.cache/huggingface/datasets/csv/default-0ff93346614de202/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7feecdce0d345b687d3d343e9fe2f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058855bd54804239998a5c26904dbaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/ck37/.cache/huggingface/datasets/csv/default-0ff93346614de202/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1404abb180545e79681832962b0af78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 1194\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 299\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "\n",
    "# Via https://huggingface.co/docs/datasets/loading_datasets.html#from-local-or-remote-files\n",
    "features = datasets.Features({\n",
    "                          'text': datasets.Value('string'),\n",
    "                          'labels': datasets.Value('string')\n",
    "                      })\n",
    "\n",
    "dataset = load_dataset('csv',\n",
    "                       data_files = {\n",
    "                           'train': str(train_file),\n",
    "                           'test': str(test_file)\n",
    "                       },\n",
    "                       skiprows = 1, # Otherwise it will treat the header as an observation.\n",
    "                       column_names = ['text', 'labels'], # Not needed, but in case we have extra columns.\n",
    "                       features = features)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badac121-f145-4f93-8c5f-6cd442174d58",
   "metadata": {},
   "source": [
    "## Specify model and associated tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088d877d-80b7-4199-8e32-6026ede756bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315808e-964c-4c80-8c04-d57ec6cbae9b",
   "metadata": {},
   "source": [
    "## Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02cfe791-34b4-4c0d-ae53-6b71f2badb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.79 s, sys: 32.2 ms, total: 1.82 s\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Via https://discuss.huggingface.co/t/converting-string-label-to-int/2816/2\n",
    "\n",
    "class_names = ['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive']\n",
    "labels = datasets.ClassLabel(names = class_names)\n",
    "labels\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch['text'], truncation = True, padding = True, max_length = 256)\n",
    "    result = labels.str2int(batch['labels'])\n",
    "    tokens['labels'] = result\n",
    "    return tokens\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize, batched = True, num_proc = 2)\n",
    "\n",
    "# Remove any extra columns to avoid a warning when training, not essential though.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n",
    "tokenized_datasets.set_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90ae3a-2f64-482d-92cc-bca8e7f72c78",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe79fcc-6f85-4840-9695-317fda79c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "# TODO: make this quieter.\n",
    "# num_labels appears to really mean num_classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                           problem_type = \"single_label_classification\",\n",
    "                                                           num_labels = len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6891e6-2a93-4742-8b63-e180a38ae273",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "This will automatically use any GPUs, provided that CUDA and pytorch are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "163191bc-312a-4da6-b3a6-363649abdf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1194\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 03:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.461700</td>\n",
       "      <td>1.218407</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.282500</td>\n",
       "      <td>1.144751</td>\n",
       "      <td>0.458194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.168400</td>\n",
       "      <td>1.131506</td>\n",
       "      <td>0.484950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.117300</td>\n",
       "      <td>0.944533</td>\n",
       "      <td>0.598662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.842348</td>\n",
       "      <td>0.712375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.910900</td>\n",
       "      <td>0.790780</td>\n",
       "      <td>0.732441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.831900</td>\n",
       "      <td>0.815294</td>\n",
       "      <td>0.742475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>0.728635</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.771100</td>\n",
       "      <td>0.676031</td>\n",
       "      <td>0.752508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>0.694619</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.668210</td>\n",
       "      <td>0.799331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.718154</td>\n",
       "      <td>0.719064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.775920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.489100</td>\n",
       "      <td>0.775512</td>\n",
       "      <td>0.772575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.769132</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 59s, sys: 35.4 s, total: 5min 35s\n",
      "Wall time: 3min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.8194707260131836, metrics={'train_runtime': 205.1923, 'train_samples_per_second': 29.095, 'train_steps_per_second': 1.828, 'total_flos': 2781862097034240.0, 'train_loss': 0.8194707260131836, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Via https://www.thepythoncode.com/code/finetuning-bert-using-huggingface-transformers-python\n",
    "training_args = \\\n",
    "    TrainingArguments(\n",
    "                      evaluation_strategy = \"steps\",\n",
    "                      warmup_steps = 100,\n",
    "                      num_train_epochs = 5, # Only need ~350 steps to converge using default LR, warmup of 100, etc.\n",
    "                      output_dir = './models/results', \n",
    "                      logging_dir = './models/logs',\n",
    "                      #weight_decay = 0.0000001,\n",
    "                      #learning_rate = 0.01,\n",
    "                      #learning_rate = 1e-2,\n",
    "                      load_best_model_at_end = True, # load the best model when finished training (default metric is loss)\n",
    "                      logging_steps = 25)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions = predictions, references = labels)\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['test'],\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbbf104a-7093-4b3c-a6f1-e0fa5ab3466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 299\n",
      "  Batch size = 16\n",
      "/home/ck37/miniforge3/envs/clinical-sentiment/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7691320776939392,\n",
       " 'eval_accuracy': 0.782608695652174,\n",
       " 'eval_runtime': 2.7505,\n",
       " 'eval_samples_per_second': 108.708,\n",
       " 'eval_steps_per_second': 6.908,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the current model after training\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b797604-61f1-4035-ad6c-8db240ccd6fa",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "846dec89-a4db-4334-baad-6b66cd293fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/config.json\n",
      "Model weights saved in /home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/pytorch_model.bin\n",
      "tokenizer config file saved in /home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/tokenizer_config.json\n",
      "Special tokens file saved in /home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/special_tokens_map.json\n",
      "added tokens file saved in /home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/added_tokens.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/tokenizer_config.json',\n",
       " '/home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/special_tokens_map.json',\n",
       " '/home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/spm.model',\n",
       " '/home/ck37/projects/clinical-sentiment-keywords/models/deberta-v3/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model_path = str(pyprojroot.here() / \"models/deberta-v3\")\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Also save trainer for future usage.\n",
    "#trainer.save_model(model_path)\n",
    "#trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7686e-855f-4396-9e41-f6e3e868cdd3",
   "metadata": {},
   "source": [
    "## Predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30e0567-88cf-4121-97f3-f1e0a2839a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text should be a single string, not a vector currently.\n",
    "def predict_sentiment(text, max_length = 512):\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    # get output probabilities by doing softmax\n",
    "    probs = outputs[0].softmax(1)\n",
    "    # executing argmax function to get the candidate label\n",
    "    return class_names[probs.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5451223f-2357-4fa0-a436-cb7dc61c8f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n",
      "Negative\n",
      "Negative\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"This is a test sentence.\"))\n",
    "print(predict_sentiment(\"I'm worried that the patient is doing poorly.\"))\n",
    "print(predict_sentiment(\"I'm extremely worried that the patient is doing terribly and will certainly die soon.\"))\n",
    "print(predict_sentiment(\"Patient's bp is normalizing, and kidney function appears to be improving.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3747f1f8-8dc3-4809-bf3c-79d455e0c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 92.9 ms, total: 1min 5s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply to full dataframe - takes 39 seconds, uses 1 GPU.\n",
    "preds = df.text.apply(predict_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7b683-3bc7-42af-8a69-d16a932e20d6",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdce90f6-f8be-4ec4-a52a-205cd680c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82940f9e-e538-4ac2-8d53-2efb713a1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds['predicted'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "638ca751-4a46-48ca-ae48-040f1c2354d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.to_excel(\"data/predicted-sentiment.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28c9dd-e1e7-402b-945d-698619e83231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clinical-sentiment]",
   "language": "python",
   "name": "conda-env-clinical-sentiment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
