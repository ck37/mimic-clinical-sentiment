{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation sample creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1602424774424,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "rNqLIArs85P1",
    "outputId": "c8d395fb-a4fe-4818-8cc3-1aaafb1a9bdc"
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "import medspacy, spacy\n",
    "from medspacy.ner import TargetRule \n",
    "from medspacy.visualization import visualize_ent\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import psutil \n",
    "num_cores = psutil.cpu_count(logical = False)\n",
    "print(\"CPU cores found:\", num_cores)\n",
    "from p_tqdm import p_map\n",
    "\n",
    "from keywords import find_keywords2\n",
    "\n",
    "#from clinical_sectionizer import TextSectionizer, Sectionizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "executionInfo": {
     "elapsed": 18565,
     "status": "ok",
     "timestamp": 1602425097162,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "q7ybOPjm86b3",
    "outputId": "13cd4645-47fa-4e84-fc0f-d3abc3b60602"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import the raw notes data.\n",
    "\n",
    "cache_file = \"data/cache/note-events.feather\"\n",
    "if not os.path.exists(cache_file):\n",
    "    # This takes a ~24 seconds to load.\n",
    "    df = pd.read_csv('data-raw/mimic/NOTEEVENTS.csv',\n",
    "                 usecols = ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CATEGORY', 'DESCRIPTION', 'CHARTTIME','CGID', 'TEXT'],\n",
    "                 low_memory = False)\n",
    "    df.to_feather(cache_file)\n",
    "else:\n",
    "    # Takes 5 seconds to load - nice speedup.\n",
    "    df = pd.read_feather(cache_file)\n",
    "\n",
    "# Lowercase the column names for easier typing.\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "print(\"Dataframe columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1602426010169,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "pHHDSer9B9Wv",
    "outputId": "1f6f781f-99bb-4a50-d9ed-13ac22a8c5e3"
   },
   "outputs": [],
   "source": [
    "# Review distribution of the notes category.\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding by note categories.\n",
    "\n",
    "exclude_categories = ['Echo', 'ECG', 'Case Management ', 'Social Work', 'Pharmacy']\n",
    "df = df[~df['category'].isin(exclude_categories)]\n",
    "print(df.category.value_counts())\n",
    "# Down to 1.8 MM notes\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df = pd.read_csv('data-raw/mimic/PATIENTS.csv',\n",
    "                     usecols = ['SUBJECT_ID', 'DOB', 'GENDER', 'EXPIRE_FLAG'],\n",
    "                     low_memory = False)\n",
    "pat_df.columns = pat_df.columns.str.lower()\n",
    "\n",
    "pat_df['birthdate'] = pd.to_datetime(pat_df['dob']).dt.date\n",
    "pat_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_df = pd.read_csv('data-raw/mimic/ADMISSIONS.csv', \n",
    "                         usecols = ['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', \n",
    "                                  'ADMISSION_TYPE', 'DISCHARGE_LOCATION', 'INSURANCE', \n",
    "                                  'MARITAL_STATUS', 'LANGUAGE', 'ETHNICITY', 'DIAGNOSIS'],\n",
    "                         low_memory = False)\n",
    "\n",
    "adm_df.columns = adm_df.columns.str.lower()\n",
    "\n",
    "adm_df['date_of_admission'] = pd.to_datetime(adm_df['admittime']).dt.date\n",
    "adm_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge patient and admission data\n",
    "adm_df = pd.merge(pat_df, adm_df, on = 'subject_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: https://mimic.mit.edu/docs/iii/tables/patients/#dob\n",
    "# Patients who are older than 89 years old at any time in the database have had their date of birth shifted to obscure their age\n",
    "adm_df['age'] = adm_df.apply(lambda e: min(round((e['date_of_admission'] - e['birthdate']).days / 365.25, 0), 90),\n",
    "                             axis = 1)\n",
    "adm_df.describe()\n",
    "adm_df.drop(['dob', 'birthdate'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict to patients >= 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge compiled patient data with notes \n",
    "# We do an inner join because we don't want admissions with no notes.\n",
    "df = pd.merge(adm_df, df, on = ['subject_id','hadm_id'], how = 'inner')\n",
    "\n",
    "# Restrict to patients >= 18\n",
    "df = df.loc[(df['age'] >= 18)]\n",
    "\n",
    "# Down to 1.25 MM notes.\n",
    "print(df.shape)\n",
    "\n",
    "# Confirm that age distribution looks good.\n",
    "df[['age']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "# Check for missingness, esp. in the text field.\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preliminary df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True).to_feather(\"data/02-notes-to-sentences-prep.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from medspacy.sentence_splitting import PySBDSenteceSplitter\n",
    "\n",
    "#sentencizer = PySBDSenteceSplitter()\n",
    "\n",
    "#import spacy\n",
    "#from pysbd.utils import PySBDFactory\n",
    "\n",
    "#nlp = medspacy.load(enable=[\"sectionizer\"])\n",
    "#print(nlp.pipe_names)\n",
    "\n",
    "# Default nlp:\n",
    "#nlp = medspacy.load()\n",
    "\n",
    "# Upgraded:\n",
    "#nlp = medspacy.load(\"en_core_sci_scibert\",\n",
    "#nlp = medspacy.load(\"en_core_sci_lg\",\n",
    "nlp = medspacy.load(load_rules = False,\n",
    "                    enable = [\"medspacy_tokenizer\"])\n",
    "#                    enable = [\"tokenizer\"],\n",
    "#                    disable=[\"target_matcher\", 'medspacy_pyrush', 'medspacy_context'])\n",
    "\n",
    "#tokenizer = medspacy.create_medspacy_tokenizer(nlp)\n",
    "#nlp.tokenizer = tokenizer\n",
    "\n",
    "# explicitly adding component to pipeline\n",
    "# (recommended - makes it more readable to tell what's going on)\n",
    "nlp.add_pipe(\"medspacy_pysbd\", first = True)\n",
    "\n",
    "print(nlp.tokenizer)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentences\n",
    "\n",
    "Create a new dataframe with one row for each sentence in a note. Each sentence should also have its count of words and characters. Don't calculate sentiment yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "print(\"Stanza version:\", stanza.__version__)\n",
    "import re\n",
    "\n",
    "# Run this the first time:\n",
    "# stanza.download('en')\n",
    "\n",
    "# Make sure to use version 1.3.0+ of stanza, otherwise you will get an md5 error here.\n",
    "# stanza.download('en', package='mimic')\n",
    "\n",
    "#nlp_stanza = stanza.Pipeline(lang = 'en',\n",
    "#                             package = 'mimic',\n",
    "#                             processors = 'tokenize')\n",
    "\n",
    "# Loop over each sentence in a note and calculate different things.\n",
    "# There are a ton of notes so this function should ideally be run in parallel.\n",
    "# Note should be a tuple.\n",
    "def analyze_note(note,\n",
    "                 newlines_to_spaces = True,\n",
    "                 remove_quotes = True,\n",
    "                 extra_clean = True,\n",
    "                 # Other options: \"stanza\"\n",
    "                 package = \"spacy\"):\n",
    "    \n",
    "    # This can raise an AttributeError() when note.text is for some reason a float.\n",
    "    note_text = note.text.strip()\n",
    "   \n",
    "    # Custom processing to improve sentence segmentation.\n",
    "    # Do this before we replace newlines to avoid clobbering a lot of text.\n",
    "    if extra_clean:\n",
    "        note_text = re.sub(r' (q\\. ?d(ay)?\\.?)', r\" qd\", note_text)\n",
    "\n",
    "        note_text = note_text.replace(\"p.o.\", \"po\")\n",
    "        note_text = note_text.replace(\"p.r.n.\", \"prn\")\n",
    "    \n",
    "    # This is important, otherwise a newline always seem to lead to a sentence break.\n",
    "    if newlines_to_spaces:\n",
    "        note_text = note_text.replace(\"\\n\", \" \")\n",
    "        note_text = note_text.replace(\"\\r\", \" \")\n",
    "        \n",
    "    if extra_clean:\n",
    "        note_text = re.sub(r'q\\.( *\\d+(\\-\\d+)?h?)\\.', r'q\\1', note_text)\n",
    "        # Long series of _____ converted to shorter word then newline.\n",
    "        note_text = re.sub('_{5,}', '\\n_LINE_\\n', note_text)\n",
    "    \n",
    "    # Only double-quotes for now, since single quotes can be used in contractions.\n",
    "    if remove_quotes:\n",
    "        note_text = note_text.replace('\"', '')\n",
    "        \n",
    "    # Extract all sentences from this note.\n",
    "    if package == \"spacy\":\n",
    "        doc = nlp(note_text)\n",
    "        sentence_iterator = doc.sents\n",
    "    elif package == \"stanza\":\n",
    "        doc = nlp_stanza(note_text)\n",
    "        sentence_iterator = doc.sentences\n",
    "\n",
    "    doc_sents = []\n",
    "    # Loop over each sentence in the note based on the sentencizer.\n",
    "    for i, sent in enumerate(sentence_iterator):\n",
    "        # Remove leading and trailing whitespace - especially newlines but also spaces.\n",
    "        sent_clean = sent.text.strip()\n",
    "        # Replace multiple spaces with a single space.\n",
    "        sent_clean = re.sub(r' +', ' ', sent_clean)\n",
    "        \n",
    "        if package == \"spacy\":\n",
    "            word_count = len(sent)\n",
    "        else:\n",
    "            word_count = len(sent.tokens)\n",
    "            \n",
    "        result = {\"row_id\": note.row_id,\n",
    "                  # Unique id for this note.\n",
    "                  \"cgid\": note.cgid,\n",
    "               \"sent_num\": i,\n",
    "                # We need to extract the raw text from the sentence object,\n",
    "                # otherwise we'll run into a \"pickling a span is not supported\" spacy error.\n",
    "               \"text\": sent_clean,\n",
    "               \"chars\": len(sent_clean),\n",
    "               \"words\": word_count}\n",
    "        # Don't calculate sentiment score in this loop - do it after all sentences have been extracted.\n",
    "        doc_sents.append(result)\n",
    "    return(pd.DataFrame(doc_sents))\n",
    "\n",
    "# Run analyze_note on each row of a df. Used when dividing a dataframe into smaller\n",
    "# partitions for parallel analysis.\n",
    "def analyze_note_df(df,\n",
    "                    min_chars = None, # 22\n",
    "                    min_words = None, # 5\n",
    "                    max_words = None, # 100\n",
    "                    **kwargs):\n",
    "    results = []\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            result = analyze_note(row, **kwargs)\n",
    "            # Join original df back onto result.\n",
    "            #result2 = pd.join(row, result)\n",
    "            results.append(result)\n",
    "        except AttributeError as e:\n",
    "            # This exception will only happen if there is no note for this admission.\n",
    "            print(\"Note.text type:\", type(row.text))\n",
    "            print(\"Note.text value:\", row.text)\n",
    "            print(e)\n",
    "    \n",
    "    combined_df = pd.concat(results)\n",
    "    # Need parentheses for these two criteria.\n",
    "    if min_words is not None:\n",
    "        combined_df = combined_df.loc[combined_df.words >= min_words]\n",
    "    if min_chars is not None:\n",
    "        combined_df = combined_df.loc[combined_df.chars >= min_chars]\n",
    "    if max_words is not None:\n",
    "        combined_df = combined_df.loc[combined_df.words <= max_words]\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-note example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "df_samp = df.sample(num_samples, random_state = 1)\n",
    "df_samp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['medspacy'] = analyze_note_df(df_samp)\n",
    "results['medspacy_label'] = analyze_note_df(df_samp,\n",
    "                                            min_chars = 22, min_words = 5, max_words = 100)\n",
    "\"\"\"\n",
    "results['stanza'] = analyze_note_df(df_samp, package = \"stanza\")\n",
    "results['stanza_label'] = analyze_note_df(df_samp, package = \"stanza\",\n",
    "                                            min_chars = 22, min_words = 5, max_words = 100)\n",
    "\"\"\"\n",
    "writer = pd.ExcelWriter('data/sentences.xls')\n",
    "\n",
    "results['medspacy'].to_excel(writer, sheet_name = 'medspacy-pysbd', index = False)\n",
    "results['medspacy_label'].to_excel(writer, sheet_name = 'medspacy-pysbd-label', index = False)\n",
    "#results['stanza'].to_excel(writer, sheet_name = 'stanza', index = False)\n",
    "#results['stanza_label'].to_excel(writer, sheet_name = 'stanza-label', index = False)\n",
    "\n",
    "df_samp[['row_id', 'category', 'text']].to_excel(writer, sheet_name = 'notes', index = False)\n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentence segmentation\n",
    "\n",
    "Takes 23 minutes with 24 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_partitions = 10000\n",
    "# This will be a list of dfs.\n",
    "df_split = np.array_split(df,\n",
    "                          #df.sample(50000),\n",
    "                          num_partitions)\n",
    "\n",
    "# Apply our function to each dataframe chunk in the list.\n",
    "result = p_map(analyze_note_df, df_split, num_cpus = num_cores)\n",
    "sent_df = pd.concat(result)\n",
    "\n",
    "sent_df.info()\n",
    "sent_df.reset_index(drop = True).to_feather(\"data/mimic-sentences-pysbd.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feather file to confirm that it was saved correctly.\n",
    "sent_df = pd.read_feather(\"data/mimic-sentences-pysbd.feather\")\n",
    "# 41 MM sentences with no note category exclusions.\n",
    "# 38 MM sentences with the note category exclusions.\n",
    "# 29 MM sentences when also excluding age < 18.\n",
    "print(sent_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential version\n",
    "\n",
    "This will take around 9 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sents = []\n",
    "\n",
    "# Analyze a random sample of notes.\n",
    "#sample_size = 1000\n",
    "#for i, row in tqdm(df.sample(sample_size).iterrows(),\n",
    "for i, row in tqdm(df.iterrows(),\n",
    "                  # total = sample_size):\n",
    "                   total = df.shape[0]):\n",
    "    sents.append(analyze_note(row))\n",
    "\n",
    "sent_df = pd.concat(sents)\n",
    "sent_df.info()\n",
    "sent_df.head()\n",
    "sent_df.reset_index(drop = True).to_feather(\"data/mimic-sentences-pysbd.feather\", index = False)\n",
    "sent_df.reset_index(drop = True).sample(2000).to_excel(\"data/mimic-sentences-pysbd-2k.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not working parallel options --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multiprocessing as mp\n",
    "\n",
    "\"\"\"\n",
    "with mp.Pool(processes = num_processes) as p:\n",
    "    # apply our function to each chunk in the list\n",
    "    result = p.imap(analyze_note_df, df_split)\n",
    "    sent_df = pd.concat(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from dask import delayed, compute\n",
    "import dask.dataframe as dd\n",
    "# Convert the pandas dataframe to a dask dataframe, to enable parallelization.\n",
    "# ddf = dd.from_pandas(df, npartitions = 10000)\n",
    "# Only analyze a random 10k rows, for testing purposes.\n",
    "ddf = dd.from_pandas(df.sample(1000), npartitions = 100)\n",
    "\n",
    "# Setup the computation that will be needed - the results will be computed in the next cell.\n",
    "sents2 = ddf.map_partitions(lambda part: part.apply(analyze_note,\n",
    "                  axis = 1), meta = result.head(0)).clear_divisions().compute() #, meta = ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df2 = pd.concat(sents2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#import swifter\n",
    "\n",
    "#sents = df.swifter.apply(lambda row: analyze_note(row), axis = 1) #, meta = ddf)\n",
    "#sents = df.apply(lambda row: analyze_note(row), axis = 1) #, meta = ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm.dask import TqdmCallback\n",
    "# Takes 23 minutes with 24 cores and 10k partitions.\n",
    "# Takes 44 minutes with 24 cores and 1k partitions.\n",
    "with TqdmCallback(desc = \"compute\"):\n",
    "    compute(sents, scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tends to be very slow for some reason.\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sents))\n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df.samples(10000), npartitions = 10000)\n",
    "\n",
    "# Setup the computation that will be needed - the results will be computed in the next cell.\n",
    "sents = ddf.apply(lambda row: analyze_note(row), axis = 1) #, meta = ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import mapply\n",
    "\n",
    "mapply.init(n_workers = -1)\n",
    "\n",
    "# Uses tqdm by default.\n",
    "sents = df.sample(1000).mapply(analyze_note, axis = 1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.concat(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvLib6eAZojVgTuuMLAZ1a",
   "collapsed_sections": [],
   "name": "MIMIC Notes Demo-Chris.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
