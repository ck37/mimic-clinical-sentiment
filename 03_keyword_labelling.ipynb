{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentences have keywords in them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1602424774424,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "rNqLIArs85P1",
    "outputId": "c8d395fb-a4fe-4818-8cc3-1aaafb1a9bdc"
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy, en_core_sci_sm, en_core_web_sm\n",
    "#from negspacy.negation import Negex\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from math import sqrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sums=pd.read_csv('data/MIMIC_sentence_discharge_summaries.csv', names=['text'], header=None)\n",
    "gen=pd.read_csv('data/MIMIC_sentence_general_notes.csv', names=['text'], header=None)\n",
    "consult=pd.read_csv('data/MIMIC_sentence_consult_notes.csv', names=['text'], header=None)\n",
    "physician=pd.read_csv('data/MIMIC_sentence_physician_notes.csv', names=['text'], header=None)\n",
    "nursing=pd.read_csv('data/MIMIC_sentence_nursing_notes.csv', names=['text'], header=None)\n",
    "resp=pd.read_csv('data/MIMIC_sentence_resp_notes.csv', names=['text'], header=None)\n",
    "rehab=pd.read_csv('data/MIMIC_sentence_rehab_notes.csv', names=['text'], header=None)\n",
    "nutrition=pd.read_csv('data/MIMIC_sentence_nutrition_notes.csv', names=['text'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords=[]\n",
    "neg_keywords=pd.read_csv('data-raw/negative_keywords.csv').iloc[::,0]\n",
    "pos_keywords=pd.read_csv('data-raw/positive_keywords.csv').iloc[::,0]\n",
    "\n",
    "all_keywords.append(neg_keywords.apply(lambda x: x.strip()).tolist())\n",
    "all_keywords.append(pos_keywords.apply(lambda x: x.strip()).tolist())\n",
    "\n",
    "keywords=[i.lower() for i in all_keywords for i in i]\n",
    "keywords.sort(key=lambda x: len(x.split()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# This file is from the google drive.\n",
    "# TODO: switch to positive/negative keyword files.\n",
    "kw_df = pd.read_excel('data-raw/Keywords for lexicon-based sentiment classifier.xlsx',\n",
    "                      sheet_name = 'Word Delineations')\n",
    "kw_df.info()\n",
    "\n",
    "# Sadly there is a space after \"negative\" and \"positive\" :/\n",
    "kw_df.columns\n",
    "\n",
    "#neg_keywords=pd.read_csv('data-raw/negative_keywords.csv').iloc[::,0]\n",
    "neg_keywords = kw_df['Negative '].dropna().str.strip().str.lower().tolist()\n",
    "\n",
    "print(neg_keywords[:5])\n",
    "\n",
    "#pos_keywords=pd.read_csv('data-raw/positive_keywords.csv').iloc[::,0]\n",
    "pos_keywords = kw_df['Positive '].dropna().str.strip().str.lower().tolist()\n",
    "print(pos_keywords[:5])\n",
    "\n",
    "kw = {'pos': pos_keywords,\n",
    "            'neg': neg_keywords,\n",
    "            'all': neg_keywords + pos_keywords}\n",
    "\n",
    "# We need to have the longer phrases first to ensure that nested keywords\n",
    "# are not detected multiple times. Sort by descending word count.\n",
    "kw['all'].sort(key = lambda x: len(x.split()), reverse = True)\n",
    "kw['all'][:5]\n",
    "keywords = kw['all']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example sentence \n",
    "text_samp='her respiratory distress was not improving, and she did not want elective intubation'\n",
    "print(text_samp)\n",
    "\n",
    "tokenized=text_samp.split()\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_words=[]\n",
    "print(text_samp)\n",
    "print('\\nKeywords Found')\n",
    "for i in keywords: \n",
    "    if re.search(r'\\b{}\\b'.format(i), text_samp):\n",
    "        print(i)\n",
    "        if i not in ' '.join(found_words):\n",
    "            found_words.append(i)\n",
    "\n",
    "print('\\nFinal list of keywords:', found_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keywords(text, keywords):\n",
    "    found_keywords=[]\n",
    "    text=str(text).lower()\n",
    "    for i in keywords: \n",
    "        if re.search(r'\\b{}\\b'.format(i), text): \n",
    "            if i not in ' '.join(found_keywords):\n",
    "                found_keywords.append(i)\n",
    "    return [text, found_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test with example sentence\n",
    "find_keywords(text_samp, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sentence that contains both \"not improving\" and \"improving\".\n",
    "test_str = \"her respiratory distress was not improving initially, but with 6L nc it is now improving.\"\n",
    "# Notice how the shorter keyword \"improving' is not detected due to the algorithm.\n",
    "find_keywords(test_str, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keywords import find_keywords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a more complex algorithm that identifies nested keywords in the text.\n",
    "# We can also use this to save the number of times each keyword appears, but taking\n",
    "# the length of the dictionary value for that keyword (i.e. the number of locations extracted).\n",
    "found_keywords = find_keywords2(test_str, keywords, verbose = True)\n",
    "print(\"All keywords found: \", found_keywords)\n",
    "list(found_keywords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word_i for word_i, locations in found_keywords.items()]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes 3 minutes.\n",
    "\n",
    "final_array=[0,0]\n",
    "for text in gen.text:\n",
    "    #temp_array=find_keywords(text, keywords)\n",
    "    initial_result = find_keywords2(text, keywords)\n",
    "    temp_array = [text, [word_i for word_i, locations in initial_result.items()]]\n",
    "    if len(temp_array[1])>0: \n",
    "        final_array=np.vstack((final_array, temp_array)) \n",
    "gen_final=pd.DataFrame(np.delete(final_array, 0, 0), columns=['text', 'keywords_found']).sample(frac=1)\n",
    "print('total gen sentences', gen.shape[0])\n",
    "print('total gen sentences with keywords', gen_final.shape[0])\n",
    "gen_final.pop('keywords_found')\n",
    "print('')\n",
    "# with found_keywords: \n",
    "# total sentences: 106,127\n",
    "# with keywords: 11,029\n",
    "# with found_keywords2:\n",
    "# total sentences:  106,127\n",
    "# with keywords:  11,029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array=[0,0]\n",
    "for text in consult.text:\n",
    "    #temp_array=find_keywords(text, keywords)\n",
    "    initial_result = find_keywords2(text, keywords)\n",
    "    temp_array = [text, [word_i for word_i, locations in initial_result.items()]]\n",
    "    if len(temp_array[1])>0: \n",
    "        final_array=np.vstack((final_array, temp_array)) \n",
    "cons_final=pd.DataFrame(np.delete(final_array, 0, 0), columns=['text', 'keywords_found']).sample(frac=1) \n",
    "print('total consult sentences', consult.shape[0])\n",
    "print('total consult sentences with keywords', cons_final.shape[0])\n",
    "cons_final.pop('keywords_found')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "final_array=[0,0]\n",
    "# CK: why is this being restricted?\n",
    "#for text in dc_sums.iloc[0:60000].text:\n",
    "for text in dc_sums.text:\n",
    "#    temp_array=find_keywords(text, keywords)\n",
    "    initial_result = find_keywords2(text, keywords)\n",
    "    temp_array = [text, [word_i for word_i, locations in initial_result.items()]]\n",
    "    if len(temp_array[1])>0: \n",
    "        final_array=np.vstack((final_array, temp_array)) \n",
    "dc_sums_final=pd.DataFrame(np.delete(final_array, 0, 0), columns=['text', 'keywords_found']).sample(frac=1)    \n",
    "print('total dc sum sentences', dc_sums.shape[0])\n",
    "print('total dc sum sentences with keywords', dc_sums_final.shape[0])\n",
    "dc_sums_final.pop('keywords_found')\n",
    "print('')\n",
    "# Previous results:\n",
    "# 5,764,750; 4,654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array=[0,0]\n",
    "# CK: why is this being restricted?\n",
    "for text in physician.iloc[0:60000].text:\n",
    "#for text in physician.text:\n",
    "#    temp_array=find_keywords(text, keywords)\n",
    "    initial_result = find_keywords2(text, keywords)\n",
    "    temp_array = [text, [word_i for word_i, locations in initial_result.items()]]\n",
    "    if len(temp_array[1])>0: \n",
    "        final_array=np.vstack((final_array, temp_array)) \n",
    "physician_final=pd.DataFrame(np.delete(final_array, 0, 0), columns=['text', 'keywords_found']).sample(frac=1)    \n",
    "print('total physician sentences', physician.shape[0])\n",
    "print('total physician sentences with keywords', physician_final.shape[0])\n",
    "physician_final.pop('keywords_found')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array=[0,0]\n",
    "for text in nursing.iloc[0:60000].text:\n",
    "    temp_array=find_keywords(text, keywords)\n",
    "    if len(temp_array[1])>0: \n",
    "        final_array=np.vstack((final_array, temp_array)) \n",
    "nursing_final=pd.DataFrame(np.delete(final_array, 0, 0), columns=['text', 'keywords_found']).sample(frac=1)    \n",
    "print('total nursing sentences', nursing.shape[0])\n",
    "print('total nursing sentences with keywords', nursing_final.shape[0])\n",
    "nursing_final.pop('keywords_found')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array=[0,0]\n",
    "for text in resp.text:\n",
    "    temp_array=find_keywords(text, keywords)\n",
    "    if len(temp_array[1])>0: \n",
    "        final_array=np.vstack((final_array, temp_array)) \n",
    "resp_final=pd.DataFrame(np.delete(final_array, 0, 0), columns=['text', 'keywords_found']).sample(frac=1)    \n",
    "print('total resp sentences', resp.shape[0])\n",
    "print('total resp sentences with keywords', resp_final.shape[0])\n",
    "resp_final.pop('keywords_found')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array=[0,0]\n",
    "for text in rehab.text:\n",
    "    temp_array=find_keywords(text, keywords)\n",
    "    if len(temp_array[1])>0: \n",
    "        final_array=np.vstack((final_array, temp_array)) \n",
    "rehab_final=pd.DataFrame(np.delete(final_array, 0, 0), columns=['text', 'keywords_found']).sample(frac=1)    \n",
    "print('total rehab sentences', rehab.shape[0])\n",
    "print('total rehab sentences with keywords', rehab_final.shape[0])\n",
    "rehab_final.pop('keywords_found')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array=[0,0]\n",
    "for text in nutrition.text:\n",
    "    temp_array=find_keywords(text, keywords)\n",
    "    if len(temp_array[1])>0: \n",
    "        final_array=np.vstack((final_array, temp_array)) \n",
    "nutrition_final=pd.DataFrame(np.delete(final_array, 0, 0), columns=['text', 'keywords_found']).sample(frac=1)    \n",
    "print('total nutrition sentences', nutrition.shape[0])\n",
    "print('total nutrition sentences with keywords', nutrition_final.shape[0])\n",
    "nutrition_final.pop('keywords_found')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sum_sample=dc_sums_final.sample(n=500).reset_index(drop=True)\n",
    "physician_sample=physician_final.sample(n=500).reset_index(drop=True)\n",
    "general_sample=gen_final.sample(n=500).reset_index(drop=True)\n",
    "consult_sample=cons_final.sample(n=406).reset_index(drop=True)\n",
    "nursing_sample=nursing_final.sample(n=500).reset_index(drop=True)\n",
    "resp_sample=resp_final.sample(n=500).reset_index(drop=True)\n",
    "rehab_sample=rehab_final.sample(n=500).reset_index(drop=True)\n",
    "nutrition_sample=nutrition_final.sample(n=500).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sum_sample.to_csv('data/MIMIC_sentence_discharge_summaries.csv', index=False)\n",
    "physician_sample.to_csv('data/MIMIC_sentence_physician_notes.csv', index=False)\n",
    "general_sample.to_csv('data/MIMIC_sentence_general_notes.csv',index=False)\n",
    "consult_sample.to_csv('data/MIMIC_sentence_consult_notes.csv',index=False)\n",
    "nursing_sample.to_csv('data/MIMIC_sentence_nursing_notes.csv',index=False)\n",
    "resp_sample.to_csv('data/MIMIC_sentence_resp_notes.csv',index=False)\n",
    "rehab_sample.to_csv('data/MIMIC_sentence_rehab_notes.csv',index=False)\n",
    "nutrition_sample.to_csv('data/MIMIC_sentence_nutrition_notes.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_final.to_csv('data/keywords_general_notes.csv')\n",
    "cons_final.to_csv('data/keywords_consult_notes.csv')\n",
    "resp_final.to_csv('data/keywords_respiratory_notes.csv')\n",
    "rehab_final.to_csv('data/keywords_rehab_notes.csv')\n",
    "nutrition_final.to_csv('data/keywords_nutrition_notes.csv')\n",
    "dc_sums_final.to_csv('data/Desktop/keywords_dc_summaries.csv')\n",
    "nursing_final.to_csv('data/Desktop/keywords_nursing_notes.csv')\n",
    "physician_final.to_csv('data/Desktop/keywords_physician_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvLib6eAZojVgTuuMLAZ1a",
   "collapsed_sections": [],
   "name": "MIMIC Notes Demo-Chris.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
