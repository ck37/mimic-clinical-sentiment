{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1602424774424,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "rNqLIArs85P1",
    "outputId": "c8d395fb-a4fe-4818-8cc3-1aaafb1a9bdc"
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Turn off FutureWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"CPU threads detected:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "executionInfo": {
     "elapsed": 18565,
     "status": "ok",
     "timestamp": 1602425097162,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "q7ybOPjm86b3",
    "outputId": "13cd4645-47fa-4e84-fc0f-d3abc3b60602"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import the raw notes data.\n",
    "# Restrict to the first 500k rows to speed this part up.\n",
    "df = pd.read_csv('NOTEEVENTS.csv', low_memory = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords=[]\n",
    "neg_keywords=pd.read_csv('negative_keywords.csv').iloc[::,0]\n",
    "pos_keywords=pd.read_csv('positive_keywords.csv').iloc[::,0]\n",
    "\n",
    "all_keywords.append(neg_keywords.apply(lambda x: x.strip()).tolist())\n",
    "all_keywords.append(pos_keywords.apply(lambda x: x.strip()).tolist())\n",
    "\n",
    "keywords=[i.lower() for i in all_keywords for i in i]\n",
    "\n",
    "neg=neg_keywords.apply(lambda x: x.strip().lower()).tolist()\n",
    "pos=pos_keywords.apply(lambda x: x.strip().lower()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df0.sample(n=100, random_state=0).reset_index(drop=True)\n",
    "\n",
    "# Lowercase the column names for easier typing.\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "print(\"Dataframe columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1602425112679,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "SVAHK6z_9afY",
    "outputId": "3cecd751-17ef-41c3-d47b-3ed455b67cf4"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1602426010169,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "pHHDSer9B9Wv",
    "outputId": "1f6f781f-99bb-4a50-d9ed-13ac22a8c5e3"
   },
   "outputs": [],
   "source": [
    "# Review distribution of the notes category - a large percentage are ECG, but nursing is 2nd most frequent.\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discharge_summaries=df[df.category=='Discharge summary'].text\n",
    "physician=df[df.category=='Physician '].text\n",
    "general=df[df.category=='General'].text\n",
    "consult=df[df.category=='Consult'].text\n",
    "nursing=df[df.category=='Nursing'].text\n",
    "respiratory=df[df.category=='Respiratory '].text\n",
    "rehab=df[df.category=='Rehab Services'].text\n",
    "nutrition=df[df.category=='Nutrition'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "digits = \"([0-9])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text) \n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences if 5<len(s)<500]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keywords(text, keywords):\n",
    "    found_keywords=[]\n",
    "    text=str(text).lower()\n",
    "    for i in keywords: \n",
    "        if re.search(r'\\b{}\\b'.format(i), text): \n",
    "            if i not in ' '.join(found_keywords):\n",
    "                found_keywords.append(i)\n",
    "    return found_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_score(keyword_list, neg_keywords, pos_keywords):\n",
    "    pos_score=len([i for i in keyword_list if i in pos_keywords])\n",
    "    neg_score=len([i for i in keyword_list if i in neg_keywords])\n",
    "    if neg_score+pos_score>0:\n",
    "        return neg_score/(neg_score+pos_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sums=pd.DataFrame(split_into_sentences(' '.join(discharge_summaries)), columns=['text'])\n",
    "physician_notes=pd.DataFrame(split_into_sentences(' '.join(physician)), columns=['text'])\n",
    "general_notes=pd.DataFrame(split_into_sentences(' '.join(general)), columns=['text'])\n",
    "consult_notes=pd.DataFrame(split_into_sentences(' '.join(consult)), columns=['text'])\n",
    "nursing_notes=pd.DataFrame(split_into_sentences(' '.join(nursing)), columns=['text'])\n",
    "resp_notes=pd.DataFrame(split_into_sentences(' '.join(respiratory)), columns=['text'])\n",
    "rehab_notes=pd.DataFrame(split_into_sentences(' '.join(rehab)), columns=['text'])\n",
    "nutrition_notes=pd.DataFrame(split_into_sentences(' '.join(nutrition)), columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 100 sample (to be consistent for medSpacy/Stanza comparison)\n",
    "dc_samp=pd.Series(dc_sums.text.iloc[0:100])\n",
    "physician_samp=pd.Series(physician_notes.text.iloc[0:100])\n",
    "general_samp=pd.Series(general_notes.text.iloc[0:100])\n",
    "consult_samp=pd.Series(consult_notes.text.iloc[0:100])\n",
    "nursing_samp=pd.Series(nursing_notes.text.iloc[0:100])\n",
    "resp_samp=pd.Series(resp_notes.text.iloc[0:100])\n",
    "rehab_samp=pd.Series(rehab_notes.text.iloc[0:100])\n",
    "nutrition_samp=pd.Series(nutrition_notes.text.iloc[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample of 100 sentences per note type (for smaller saved file)\n",
    "dc_samp.to_csv('MIMIC_sentence_discharge_summaries.csv', index=False)\n",
    "physician_samp.to_csv('MIMIC_sentence_physician_notes.csv', index=False)\n",
    "general_samp.to_csv('MIMIC_sentence_general_notes.csv',index=False)\n",
    "consult_samp.to_csv('MIMIC_sentence_consult_notes.csv',index=False)\n",
    "nursing_samp.to_csv('MIMIC_sentence_nursing_notes.csv',index=False)\n",
    "resp_samp.to_csv('MIMIC_sentence_resp_notes.csv',index=False)\n",
    "rehab_samp.to_csv('MIMIC_sentence_rehab_notes.csv',index=False)\n",
    "nutrition_samp.to_csv('MIMIC_sentence_nutrition_notes.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue here for isolation of keywords/keyword score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random sampling\n",
    "dc_sum_sample=dc_sums.sample(n=1000).reset_index(drop=True)\n",
    "physician_sample=physician_notes.sample(n=1000).reset_index(drop=True)\n",
    "general_sample=general_notes.sample(n=1000).reset_index(drop=True)\n",
    "consult_sample=consult_notes.sample(n=1000).reset_index(drop=True)\n",
    "nursing_sample=nursing_notes.sample(n=1000).reset_index(drop=True)\n",
    "resp_sample=resp_notes.sample(n=1000).reset_index(drop=True)\n",
    "rehab_sample=rehab_notes.sample(n=1000).reset_index(drop=True)\n",
    "nutrition_sample=nutrition_notes.sample(n=1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sum_sample['keywords']=dc_sum_sample.text.map(lambda x: find_keywords(x, keywords))\n",
    "physician_sample['keywords']=physician_sample.text.map(lambda x: find_keywords(x, keywords))\n",
    "general_sample['keywords']=general_sample.text.map(lambda x: find_keywords(x, keywords))\n",
    "consult_sample['keywords']=consult_sample.text.map(lambda x: find_keywords(x, keywords))\n",
    "nursing_sample['keywords']=nursing_sample.text.map(lambda x: find_keywords(x, keywords))\n",
    "resp_sample['keywords']=resp_sample.text.map(lambda x: find_keywords(x, keywords))\n",
    "rehab_sample['keywords']=rehab_sample.text.map(lambda x: find_keywords(x, keywords))\n",
    "nutrition_sample['keywords']=nutrition_sample.text.map(lambda x: find_keywords(x, keywords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sum_sample['keyword_score']=dc_sum_sample.keywords.map(lambda x: keyword_score(x, neg, pos))\n",
    "physician_sample['keyword_score']=physician_sample.keywords.map(lambda x: keyword_score(x, neg, pos))\n",
    "general_sample['keyword_score']=general_sample.keywords.map(lambda x: keyword_score(x, neg, pos))\n",
    "consult_sample['keyword_score']=consult_sample.keywords.map(lambda x: keyword_score(x, neg, pos))\n",
    "nursing_sample['keyword_score']=nursing_sample.keywords.map(lambda x: keyword_score(x, neg, pos))\n",
    "resp_sample['keyword_score']=resp_sample.keywords.map(lambda x: keyword_score(x, neg, pos))\n",
    "rehab_sample['keyword_score']=rehab_sample.keywords.map(lambda x: keyword_score(x, neg, pos))\n",
    "nutrition_sample['keyword_score']=nutrition_sample.keywords.map(lambda x: keyword_score(x, neg, pos))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_sum_sample['keyword_presence']=dc_sum_sample.keywords.map(lambda x: True if len(x)>0 else False)\n",
    "physician_sample['keyword_presence']=physician_sample.keywords.map(lambda x: True if len(x)>0 else False)\n",
    "general_sample['keyword_presence']=general_sample.keywords.map(lambda x: True if len(x)>0 else False)\n",
    "consult_sample['keyword_presence']=consult_sample.keywords.map(lambda x: True if len(x)>0 else False)\n",
    "nursing_sample['keyword_presence']=nursing_sample.keywords.map(lambda x: True if len(x)>0 else False)\n",
    "resp_sample['keyword_presence']=resp_sample.keywords.map(lambda x: True if len(x)>0 else False)\n",
    "rehab_sample['keyword_presence']=rehab_sample.keywords.map(lambda x: True if len(x)>0 else False)\n",
    "nutrition_sample['keyword_presence']=nutrition_sample.keywords.map(lambda x: True if len(x)>0 else False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All sentences for labelling\n",
    "dc_sums.to_csv('MIMIC_sentence_discharge_summaries.csv', index=False)\n",
    "physician_notes.to_csv('MIMIC_sentence_physician_notes.csv', index=False)\n",
    "general_notes.to_csv('MIMIC_sentence_general_notes.csv',index=False)\n",
    "consult_notes.to_csv('MIMIC_sentence_consult_notes.csv',index=False)\n",
    "nursing_notes.to_csv('MIMIC_sentence_nursing_notes.csv',index=False)\n",
    "resp_notes.to_csv('MIMIC_sentence_resp_notes.csv',index=False)\n",
    "rehab_notes.to_csv('MIMIC_sentence_rehab_notes.csv',index=False)\n",
    "nutrition_notes.to_csv('MIMIC_sentence_nutrition_notes.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous Code don't use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr=np.zeros(2)\n",
    "for note in range(len(consult)):\n",
    "    fragments=split_sentence(consult.iloc[note])\n",
    "\n",
    "    for frag in range(len(fragments)): \n",
    "        #row=np.array()\n",
    "        row=np.hstack((note, fragments[frag]))\n",
    "        #print(row)\n",
    "        temp_arr=np.vstack((temp_arr, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_split_sentence(text):\n",
    "    combined=[]\n",
    "    split_text=re.split(r'\\. ', text.replace('\\n', ' '))\n",
    "    \n",
    "    if len(split_text)%2>0: \n",
    "        remainder=len(split_text)%2\n",
    "        count=0\n",
    "        while count<((len(split_text)//3)+remainder): \n",
    "            combined.append('. '.join(split_text[count:count+3]))\n",
    "            count+=1\n",
    "    else: \n",
    "        for i in range(len(split_text)//3): \n",
    "            combined.append('. '.join(split_text[i:i+3]))\n",
    "        \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr=[0,0]\n",
    "for note in range(len(test_dc)):\n",
    "    fragments=three_split_sentence(test_dc.iloc[note])\n",
    "\n",
    "    for frag in range(len(fragments)): \n",
    "        #row=np.array()\n",
    "        row=np.hstack((note, fragments[frag]))\n",
    "        #print(row)\n",
    "        temp_arr=np.vstack((temp_arr, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_sentence_sample=pd.DataFrame(np.delete(temp_arr,0,0), columns=['patient_id', 'three_sentence'])\n",
    "print(three_sentence_sample.shape)\n",
    "three_sentence_sample.head(5)\n",
    "three_sentence_sample.to_csv('~/three_sentence_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sectional headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_headers_and_text(text):\n",
    "    #print(text_spaced[0:50])\n",
    "    combined=[]\n",
    "    split_text=[i for i in re.split(r'(.*:)', text)]\n",
    "    #print(len(split_text))\n",
    "    for i in range(len(split_text)): \n",
    "        #print(split_text[i][-1::])\n",
    "        if split_text[i][-1::]==':':\n",
    "            combined.append(split_text[i]+ ' // ' + split_text[i+1])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_arr=[0,0]\n",
    "for note in range(len(test_dc)):\n",
    "    fragments=combine_headers_and_text(test_dc.iloc[note])\n",
    "    #print(len(fragments))\n",
    "    #print('')\n",
    "    for frag in range(len(fragments)): \n",
    "        #row=np.array()\n",
    "        row=np.hstack((note, fragments[frag]))\n",
    "        #print(row)\n",
    "        temp_arr=np.vstack((temp_arr, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blocked_text_sample=pd.DataFrame(np.delete(temp_arr,0,0), columns=['patient_id', 'blocked_text'])\n",
    "print(blocked_text_sample.shape)\n",
    "blocked_text_sample.head(5)\n",
    "blocked_text_sample.to_csv('~/blocked_text_sample.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvLib6eAZojVgTuuMLAZ1a",
   "collapsed_sections": [],
   "name": "MIMIC Notes Demo-Chris.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
