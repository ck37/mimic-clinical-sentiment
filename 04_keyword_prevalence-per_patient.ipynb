{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1602424774424,
     "user": {
      "displayName": "Chris Kennedy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiD_hpxe07s1ciHg3m-GksY10iiuhNWK9FAqpL_IntRUJH4IJTK2KROhyVoN6ngjs4-KERVCTDJcpZtiVs2ErG7qhUASKB8scusryurMLLDzZDXpQLxuH9-CCtpqQSQ1NBLp2H962Z7mjug3o_forRggl4VokvJCSCoVipyOrhTI5xWllPt3JHBM6gkV-tkyLMcfq0PZPPwNkKDs4ZBcC8NSn5rTDwQ-2LYJ6rVGagX_X35oPj8TxmR-Q3V2ltd-CGHsudc5joVscRnWg6aKJ4M-rXQ4OQMpPx_GbYaSreiDGEonZaS-ZJj9PxkjPAcHsJ_k8SoQixCVKeQcPbDHW7GSbmshoRBdWmcyvWbSVzhmgGccTDKwLdUfBrVSu0-QdboOKLLncEtLjclTrmO9413uPQbpiTdQ_kLd8FGuK-6MHyOv0fGcywY_F0dH4fMAbgOija0CZEGmnHo1KQ9BPyJREvHrWGrgSTFeISsr8tovTvzwMGyLPc49Gz88e9wRnAFm5lMdr3x4YkjT2KhP-AliAzqlpr0idEyyCbzlD8MsnUgbdRh600FQmWcEJzPJ8KgMXpO3rtkRzGGr0dxHjvzRKfp47ScSEZe3iQeCsvpQKafLvG7cTqcwXhy2cT5fkp-d6Hxr10oXgOOaR5UfW0XoF9gXbf4xcCdZHGrqmVjlrHNU0X-yoaGQXqd5HAr9YlM0IT1T6jnoK6hzyXnRLFk_UXZsWcvQUI1fcjd0x0vtgqBRE6-n8Bsloz-4rnddIDj9Q=s64",
      "userId": "05423414885832015580"
     },
     "user_tz": 240
    },
    "id": "rNqLIArs85P1",
    "outputId": "c8d395fb-a4fe-4818-8cc3-1aaafb1a9bdc"
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Created by 02_merge_data_keywords_per_patient_with_outcomes_4.20.ipynb\n",
    "all_patients0 = pd.read_csv('data/MIMIC_001_merge_patients_4.20.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41k\n",
    "all_patients0.SUBJECT_ID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove discharge summary notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_patients0.shape)\n",
    "all_patients = all_patients0[all_patients0.CATEGORY!='Discharge summary']\n",
    "print(all_patients.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping discharge summaries is what brings us down to only 7.7k patients.\n",
    "all_patients.SUBJECT_ID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sentiment keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_patients[['CHARTTIME', 'TEXT']]\n",
    "# Just for comparison to find_keywords2\n",
    "def find_keywords(text, keywords):\n",
    "    found_keywords=[]\n",
    "    text=str(text).lower()\n",
    "    for i in keywords: \n",
    "        if re.search(r'\\b{}\\b'.format(i), text): \n",
    "            if i not in ' '.join(found_keywords):\n",
    "                found_keywords.append(i)\n",
    "    return [text, found_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load keywords and combine pos+neg keywords together \n",
    "# \"\"\"\n",
    "all_keywords=[]\n",
    "# Files are downloaded from https://drive.google.com/drive/u/0/folders/1ZdED366Wa2Hrbd2VSz7zbvRJ2iJZ1S81\n",
    "# Dated November 29, 2020\n",
    "neg_keywords=pd.read_csv('data-raw/negative_keywords.csv').iloc[::,0]\n",
    "pos_keywords=pd.read_csv('data-raw/positive_keywords.csv').iloc[::,0]\n",
    "\n",
    "all_keywords.append(neg_keywords.apply(lambda x: x.strip()).tolist())\n",
    "all_keywords.append(pos_keywords.apply(lambda x: x.strip()).tolist())\n",
    "\n",
    "keywords=[i.lower() for i in all_keywords for i in i]\n",
    "keywords.sort(key=lambda x: len(x.split()), reverse=True)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is from the google drive.\n",
    "\"\"\"\n",
    "kw_df = pd.read_excel('data-raw/Keywords for lexicon-based sentiment classifier.xlsx',\n",
    "                      sheet_name = 'Word Delineations')\n",
    "kw_df.info()\n",
    "\n",
    "# Sadly there is a space after \"negative\" and \"positive\" :/\n",
    "kw_df.columns\n",
    "\n",
    "#neg_keywords=pd.read_csv('data-raw/negative_keywords.csv').iloc[::,0]\n",
    "neg_keywords = kw_df['Negative '].dropna().str.strip().str.lower().tolist()\n",
    "\n",
    "print(neg_keywords[:5])\n",
    "\n",
    "#pos_keywords=pd.read_csv('data-raw/positive_keywords.csv').iloc[::,0]\n",
    "pos_keywords = kw_df['Positive '].dropna().str.strip().str.lower().tolist()\n",
    "print(pos_keywords[:5])\n",
    "\n",
    "kw = {'pos': pos_keywords,\n",
    "            'neg': neg_keywords,\n",
    "            'all': neg_keywords + pos_keywords}\n",
    "\n",
    "# We need to have the longer phrases first to ensure that nested keywords\n",
    "# are not detected multiple times. Sort by descending word count.\n",
    "kw['all'].sort(key = lambda x: len(x.split()), reverse = True)\n",
    "kw['all'][:5]\n",
    "keywords = kw['all']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column of TEXT that is split per sentence \n",
    "# TODO: use a sentence segmentation algorithm rather than this simpler version.\n",
    "all_patients['TEXT_SPLIT'] = all_patients.TEXT.map(lambda x: x.split('. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keywords import find_keywords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes ~45 minutes\n",
    "\n",
    "# TODO: convert to multicore somehow.\n",
    "#all_patients['FOUND_KEYWORDS']=all_patients.TEXT_SPLIT.map(lambda x: find_keywords_final(x)[1])\n",
    "all_patients['FOUND_KEYWORD_DETAILS'] = all_patients.TEXT_SPLIT.map(lambda x: find_keywords2(x, keywords))\n",
    "all_patients['FOUND_KEYWORD_DETAILS'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Extract each keyword everytime that it occurs, and flatten the resulting nested list\n",
    "all_patients['FOUND_KEYWORD_REPEATS'] = all_patients['FOUND_KEYWORD_DETAILS'].map(lambda x: list(itertools.chain.from_iterable([[key_i] * len(locations) for key_i, locations in x.items()])))\n",
    "# This version doesn't count how many times the keyword occurs.\n",
    "all_patients['FOUND_KEYWORD_UNIQUE'] = all_patients['FOUND_KEYWORD_DETAILS'].map(lambda x: x.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients['FOUND_KEYWORD_REPEATS'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD\n",
    "#all_patients['FOUND_KEYWORDS'] = all_patients.TEXT_SPLIT.map(lambda x: list(find_keywords2(x, keywords).keys()))\n",
    "# all_patients['FOUND_KEYWORDS'] = all_patients.TEXT_SPLIT.map(lambda x: list(find_keywords2(x, keywords).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients.TEXT_SPLIT.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_keywords2(all_patients.TEXT_SPLIT.values[0], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_keywords(all_patients.TEXT_SPLIT.values[0], keywords)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_patients['FOUND_KEYWORDS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_patients.FOUND_KEYWORDS.map(lambda x: ', '.join(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This takes 24 minutes sadly.\n",
    "# all_patients['ORIG_SENTENCE_WITH_KEYWORD']=all_patients.TEXT_SPLIT.map(lambda x: find_keywords(x, keywords)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_patients['NUM_SENTENCES_WITH_KEYWORDS']=all_patients.FOUND_KEYWORDS.map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_keywords=[i.strip().lower() for i in neg_keywords]\n",
    "positive_keywords=[i.strip().lower() for i in pos_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_patients['FOUND_KEYWORD_REPEATS'][2])\n",
    "print(\"Negative keywords:\", sum([kw in negative_keywords for kw in all_patients['FOUND_KEYWORD_REPEATS'][2]]))\n",
    "print(\"Positive keywords:\", sum([kw in positive_keywords for kw in all_patients['FOUND_KEYWORD_REPEATS'][2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients['NUM_POS_KEYWORDS']=all_patients.FOUND_KEYWORD_REPEATS.map(lambda x: sum([kw in positive_keywords for kw in x]))\n",
    "all_patients['NUM_NEG_KEYWORDS']=all_patients.FOUND_KEYWORD_REPEATS.map(lambda x: sum([kw in negative_keywords for kw in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#all_patients['AGGR_KEYWORDS']=all_patients.FOUND_KEYWORDS.map(lambda x: [a for a in x for a in a])\n",
    "\n",
    "# TODO: this needs to count keywords that occur multiple times, not just once.\n",
    "\"\"\"\n",
    "all_patients['NUM_POS_KEYWORDS']=all_patients.FOUND_KEYWORDS.map(lambda x: len([i for i in x if i in positive_keywords]))\n",
    "all_patients['NUM_NEG_KEYWORDS']=all_patients.FOUND_KEYWORDS.map(lambda x: len([i for i in x if i in negative_keywords]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not being used.\n",
    "all_patients['FOUND_STRING_KEYWORDS']=all_patients.FOUND_KEYWORD_REPEATS.map(lambda x: ', '.join(x)) \n",
    "#final_df=pd.concat([all_patients.drop('AGGR_STRING_KEYWORDS',axis=1), all_patients['AGGR_STRING_KEYWORDS'].str.get_dummies(sep=', ')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients.head(5)\n",
    "print('total number of patients:', len(all_patients.SUBJECT_ID.unique().tolist()))\n",
    "print('total hospital encounters:', len(all_patients.HADM_ID.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save demographics through up to aggregate keywords (i.e. no keyword dummies)\n",
    "all_patients.to_csv('data/keyword_prevalence_per_patient_nodummies_10.12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients.shape\n",
    "# Dropped to 7.7k patients :(\n",
    "all_patients.SUBJECT_ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subgroup of patients >=65 years old \n",
    "patients_65=all_patients[all_patients.AGE>=65]\n",
    "print(patients_65.shape)\n",
    "patients_65.to_csv('data/keyword_prevalence_per_patient_nodummies_age65.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes 73 seconds.\n",
    "df_kw_counts = all_patients.FOUND_KEYWORD_REPEATS.apply(pd.Series.value_counts).fillna(0).astype(int)\n",
    "df_kw_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final_df adds columns where each keyword is a dummy variable \n",
    "#final_df=pd.concat([all_patients.drop('FOUND_STRING_KEYWORDS',axis=1),\n",
    "#                    all_patients['FOUND_STRING_KEYWORDS'].str.get_dummies(sep=', ')], axis=1)\n",
    "\n",
    "# CK: do we need to reset_index or something?\n",
    "final_df = pd.concat([all_patients, df_kw_counts], axis = 1)\n",
    "final_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total number of patients:', len(final_df.SUBJECT_ID.unique().tolist()))\n",
    "print('total hospital encounters:', len(final_df.HADM_ID.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('data/keyword_prevalence_per_patient_10.12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducing all the keywords used/found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/keyword_prevalence_per_patient_10.12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_keywords=[i.strip().lower() for i in neg_keywords]\n",
    "positive_keywords=[i.strip().lower() for i in pos_keywords]\n",
    "df.FOUND_KEYWORDS.map(lambda x: len([ negative_keywords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.FOUND_KEYWORDS.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in df.FOUND_KEYWORDS.iloc[0] if i in positive_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_keywords=df[['TEXT', 'FOUND_KEYWORDS']]\n",
    "just_keywords.to_csv('data/list_of_keywords_found_per_note.csv', nrows=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_keywords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvLib6eAZojVgTuuMLAZ1a",
   "collapsed_sections": [],
   "name": "MIMIC Notes Demo-Chris.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
